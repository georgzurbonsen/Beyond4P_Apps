	include ( Style Library );
//	runtime settings[verbose] = quiet;

	// Some parameter settings you can vary

	limit[contents]   = 20;  // Show at most 20 data entries for every header
	limit[char count] = 32;  // Limit displaying contents to max 32 chars.  Add '...' if contents is longer and needs to be truncated.
	limit[shrinking]  = 0.8; // Only consolidating same contents shrinks to less than factor specified, then the top 20 will be listed.  Not the case if max 20 rows exist.


	// Following function will analyze one table column and add gathered information into the table name 'Report'

	define procedure( analyze json level, {{ level, numeral }} )
	{

	    table copy table selected rows 		( Table, 1 Level, [JSON Level]=level[] );
	    table describe selected rows ignore case	( 1 Level, ([JSON Type]<>void,array,object,boolean), JSON Value, Info Type, Lookup, Check, Description );
	    table process  selected rows		( 1 Level, ([JSON Type]= void,array,object,boolean), [Info Type] = [JSON Type] );

	    JSON types[] = join(+trim([1 Level:JSON Type,..]), ", ");

	    [Report:{Level,JSON Type},table length(Report)]	= { level[], JSON types[] };

	    for all parameters ( trim ( [1 Level:JSON Name,..] ) / {''}, object name[] )
	    {
		echo("    Process object name: ", object name[] );
		[Report:Object Name,table length(Report)] = object name[];
	        copy					( table length(Report), row 1[], row 2[], row 3[] );	// Simple! Assigning length into 3 variables.

	        table copy table selected rows		( 1 Level, Copy, [JSON Name]==object name[] );
	        table insert columns			( Copy, "Count" );

	        table consolidate			( Copy, JSON Value, Count, count );				// Consolidate repeated data contents
	        table sort rows				( Copy, {Info Type, Count}, {alphabetic, numeric down} );
	        table insert columns			( Copy, Count 2 );


		table copy table columns		( Copy, Info Type Summary, { JSON Type, Info Type, Count, Count 2 }  );	// Create a short list of diff. info types
		table consolidate			(       Info Type Summary, Info Type, {Count, Count 2}, {sum, count} );

		table insert columns			( Info Type Summary, Shrink Factor );
		table process				( Info Type Summary, [Shrink Factor] = pick if( [Count]>0, [Count 2]/[Count], 0 ); if ([Count 2]<=limit[contents]) [Shrink Factor] = 0 );

		// Special meaning:			Shrink factor = 0 means list all contents.

		// The tabe 'Info Type Summary' lists the info types, the counts in the original column and the counts in the column without repeated contents.
		// With this data, start listing, top down, most frequent values.


		for all table rows			( Info Type Summary, Info Type, info type[] )
		{
		    [Report:{JSON Type,Info Type, Info Type Count},row 1[]++] = { [JSON Type],info type[], [Count] };
		    table copy table selected rows( Copy, Copy 2, [Info Type]==info type[] );
		
		    count[] = 0;
		    if ( [Shrink Factor] < limit[shrinking] )
		    {
			table process selected rows( Copy 2, row() <= min( [^Count 2], limit[contents] ), with table( Report, row 1[]++)
			{
			    count[]			+=[^Count];
			    [Contents Count]		= [^Count];
			    [Contents]			= left(str([^JSON Value]),limit[char count]) + select if( str([^JSON Value]){}>limit[char count]," ...","" );
			} );
		    }

		    if (table length( Copy 2 )-1 > limit[contents]) with table( Report, row 1[]++ )
		    {
			[Contents]		= select if (count[]==0, "... Highly diverse contents ... :", "... Other contents ... :" );
			[Contents Count]	= [^Count] - count[]; // Header not to count.
		    }
		}


		total count[] = sum([Info Type Summary:Count  ,..]);
		conso count[] = sum([Info Type Summary:Count 2,..]);

		[Report:{Statistics, Statistics Value},row 2[]++] = {"# Repeated contents", total count[] - conso count[] };
		[Report:{Statistics, Statistics Value},row 2[]++] = {"Common contents ratio", pick if( total count[]==0, 0, 1-conso count[]/total count[]) };

		count[]	= table copy columns selected rows( Copy, ([Info Type]=integer,numeric,scientific), 0, Numeric Value );
		values[]	= [Copy:Numeric Value,..];
		if (count[] > 0) 
		{
		    [Report:{Statistics, Statistics Value},row 2[]++] = {"Smallest number", min ignore blanks( values[] ) };
		    [Report:{Statistics, Statistics Value},row 2[]++] = {"Average",         average ignore blanks( values[] ) };
		    [Report:{Statistics, Statistics Value},row 2[]++] = {"Biggest number",  max 123( values[] ) };
		}

	    }

	} // Procedure definition done.


	// Main program begins here:

	watch start;
//	table initialize		( Report, {{ File Name, Sheet Name, Table Length, Table Width,        Header Name, JSON Type, Info Type, Info Type Count, Contents, Contents Count, Statistics, Statistics Value }} );
	table initialize		( Report, {{ File Name,             Data Count,   Data Depth,  Level, Object Name, JSON Type, Info Type, Info Type Count, Contents, Contents Count, Statistics, Statistics Value }} );
	table load excel file		( Lookup, "Lookup.xlsx" );
	
	for all parameters( search files( ".", "*.json" ), file name[] ) if (file name[] <> '') // Note the files to ignore.  ... <> '' : At present, nothing to ignore.
	{
		table load		( Table, file name[], JSON );
		json depth[]		= max 123([Table:JSON Level,..]);
		json leaf count[]	= count if( [Table:JSON Type,..], [Table:JSON Type,..], (<>'array','object') );
		json array count[]	= count if( [Table:JSON Type,..], [Table:JSON Type,..], array );
		json object count[]	= count if( [Table:JSON Type,..], [Table:JSON Type,..], object );
		echo			("File Name: ", file name[], "  JSON hierarchical depth: ", json depth[],  "  JSON data count: ",json leaf count[],",");
		echo			("           ", json array count[], " array(s) and ", json object count[], " object(s).");
		table append		( Report, {{file name[], json leaf count[], json depth[] }} );

		for (level[] = 0, level[] <= json depth[], level[]++)
		{
		    echo("    Hierarchy level: ", level[] );
		    analyze json level( level[] );
		}

	}



	// Round up: Add some style such as autofilter, freeze top row and highlight next files and columns.  Then save the results.
	// Base Data is suitable for visualization in Excel

	table copy table			( Report, Base Data );
	table fill vertically selected rows	( Base Data, [File Name]=='', {Level, Object Name} );
	table fill vertically			( Base Data, { File Name, Level } );
	table fill vertically selected rows	( Base Data, [Info Type Count]!='' | [Contents Count]!='', Info Type );

	table process selected rows		( Base Data, [Info Type Count]!='', [Info Type] += " total" );
	table style table			( Base Data, sheet, freeze rows, 1, autofilter, 0 );

	table style table			( Base Data, sheet, freeze rows, 1, autofilter, 0 );
	table style columns			( Base Data, 0, sheet, column width, 30 );
	table style auto width			( Base Data );

	table style auto width			( Report );
	table style columns			( Report, 0, sheet, column width, 30 );
	table style table			( Report, sheet, freeze rows, 1, autofilter, 0 );
	table process all selected rows 	( Report, [File Name]!='', table style rows( Report, row(), table, boldface, true, fill color, gray 11 ) );
	table process all selected rows 	( Report, [Level]!='', table style rows( Report, row(), table, boldface, true, fill color, gray 13 ) );
	table process all selected rows 	( Report, [Object Name]!='', table style rows( Report, row(), table, fill color, gray 15 ) );
	table save excel file			( { Report, Base Data }, { Report, Base Data }, Report.xlsx );

	echo("Finished processing.  Time (s): ", watch stop()/1000 );
